{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "Takes trained model (from Luke).\n",
    "\n",
    "On each \"trial\" takes 1 sample + N(e.g. 20) test, where exactly one of the 20 is the same class as in the sample.\n",
    "\n",
    "Finds the 1 in the N that matches the sample the best, based on the posterior predictive probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdata_numpy = np.fromfile(\"examples/mnist/data/binarized_mnist_test.amat\", dtype=np.int16).reshape(-1,1,28,28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) get encodings for all train characters\n",
    "\n",
    "2) build model using those training characters\n",
    "\n",
    "3) classify test characters\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import examples.mnist as M\n",
    "import numpy as np\n",
    "\n",
    "model=torch.load(\"./model.p\", map_location='cpu') #LT\n",
    "print(\"Loaded model.p\")\n",
    "\n",
    "\n",
    "\n",
    "## ==== REPLACE M.testdata WITH TORCHVISION MNIST DATA, so that have accurate labels.\n",
    "# scale by 255, sample from bernoulli with those p, in order to binarize.\n",
    "\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "mnist_test = datasets.MNIST(root='./data', train=False, download=True)\n",
    "mnist_test.test_data = mnist_test.test_data.reshape(-1, 1, 28, 28)\n",
    "\n",
    "# ===== BINARIZES THE DATASET\n",
    "from torch.distributions.bernoulli import Bernoulli\n",
    "mnist_bin = Bernoulli(mnist_test.test_data.to(torch.float64)/255).sample()\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(121)\n",
    "plt.imshow(mnist_test.test_data.to(torch.float64)[1][0])\n",
    "plt.subplot(122)\n",
    "plt.imshow(mnist_bin[1][0])\n",
    "\n",
    "mnist_test.test_data = mnist_bin\n",
    "\n",
    "mnist_test.test_data.to(torch.uint8)\n",
    "mnist_test.test_data[0][0]\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(121)\n",
    "plt.imshow(mnist_test.test_data[1][0])\n",
    "\n",
    "# ========= PERFORM REPLACEMENT\n",
    "n = M.testdata.shape[0]\n",
    "M.testdata = mnist_test.test_data[0:n]\n",
    "M.testlabels = mnist_test.test_labels[0:n]\n",
    "\n",
    "\n",
    "## ===== COMPARE original binarized dataset (above, train) vs. one that I made (below, test)\n",
    "\n",
    "plt.figure(figsize=(10,20))\n",
    "indthis = np.random.randint(low=0, high=100, size=10)\n",
    "for i, ind in enumerate(indthis):\n",
    "    plt.subplot(1,10,i+1)\n",
    "    plt.imshow(M.data[ind][0].numpy().reshape(28, 28), vmin=0, vmax=1)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "plt.figure(figsize=(10,20))\n",
    "for i, ind in enumerate(indthis):\n",
    "    plt.subplot(1,10,i+1)\n",
    "    plt.imshow(M.testdata[ind][0].numpy().reshape(28, 28), vmin=0, vmax=1)\n",
    "##    plt.tight_layout()\n",
    "\n",
    "# Classification code below\n",
    "\n",
    "## define a function that loads appropriate characters\n",
    "\n",
    "def getIdx(charSamp, M, Nway=20):    \n",
    "#     charSamp = 1 # the sample integer (0,...,9)\n",
    "#     Nway = 20 # how many test characters (only 1 will match the Samp)\n",
    "\n",
    "    charSamp = torch.tensor(np.array(charSamp))\n",
    "    \n",
    "    # get sample\n",
    "    a = np.where(M.testlabels==charSamp)\n",
    "    idx_sample, idx_testmatch = np.random.choice(a[0], size=2, replace=False)\n",
    "\n",
    "    # get the N-1 test that do not match the sample\n",
    "    a = np.where(M.testlabels!=charSamp)\n",
    "    idx_testnonmatch = np.random.choice(a[0], size=Nway-1, replace=False)\n",
    "    \n",
    "    return idx_sample, idx_testmatch, idx_testnonmatch\n",
    "\n",
    "\n",
    "idx_sample, idx_testmatch, idx_testnonmatch = getIdx(1, M)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "charSamp = 4\n",
    "Nway=10\n",
    "\n",
    "## FUNCTION to computer posteriors\n",
    "idx_sample, idx_testmatch, idx_testnonmatch = getIdx(charSamp = charSamp, M=M, Nway=Nway)\n",
    "# concatenate them so that can run all together\n",
    "idx_all = np.concatenate((idx_sample.reshape(1), idx_testmatch.reshape(1), idx_testnonmatch))\n",
    "\n",
    "# get latent for 1-sample\n",
    "c_all, _ = model.encoder(i=np.arange(len(idx_all)), x=M.testdata[idx_all])\n",
    "# c_all, _ = model.encoder(i=np.arange(len(idx_all)), x=M.testdata[idx_all])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# likelihoods for N-tests - HERE: EACH CHAR USING ITS OWN MODEL.\n",
    "noiseval = 0.25\n",
    "sample_probs=True\n",
    "NiterNoise = 5\n",
    "score_all = []\n",
    "for n in range(NiterNoise):\n",
    "    x_decod, score = model.decoder(i=np.arange(len(idx_all)), c=c_all, x=M.testdata[idx_all], \n",
    "                             noise=noiseval, sample_probs=sample_probs)\n",
    "    score_all.append(score.detach().numpy())\n",
    "\n",
    "score_all = [ss.reshape(1,-1) for ss in score_all]\n",
    "score_all = np.concatenate(score_all)\n",
    "\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.plot(score_all.T, '-o')\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "for j, xx in enumerate(M.testdata[idx_all]):\n",
    "    plt.subplot(1,len(idx_all),j+1)\n",
    "    plt.title('orig%s' % j)\n",
    "    plt.imshow(xx.numpy().reshape(28, 28), vmin=0, vmax=1)   \n",
    "    \n",
    "plt.figure(figsize=(20,5))\n",
    "for j, xx in enumerate(x_decod):\n",
    "    plt.subplot(1,len(idx_all),j+1)\n",
    "    plt.title('decod%s' % j)\n",
    "    plt.imshow(xx.detach().numpy().reshape(28, 28), vmin=0, vmax=1)   \n",
    "    \n",
    "\n",
    "# -underlay with the images of the characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DIFFERENT - HERE, MODEL FOR THE FIRST CHARACTER, AGAINST THE OTHERS.\n",
    "# This illustrates the problem:\n",
    "# the image that is decoded using model.decoder() is entirely derived from the c that are input. \n",
    "c_onlyfirst = [c[0] for _ in range(len(idx_all))]\n",
    "\n",
    "\n",
    "noiseval = 0\n",
    "sample_probs=True\n",
    "NiterNoise = 5\n",
    "score_all = []\n",
    "for n in range(NiterNoise):\n",
    "    x_decod, score = model.decoder(i=np.arange(len(idx_all)), c=c_onlyfirst, x=M.testdata[idx_all], \n",
    "                                   noise=noiseval, sample_probs=sample_probs)\n",
    "    score_all.append(score.detach().numpy())\n",
    "\n",
    "score_all = [ss.reshape(1,-1) for ss in score_all]\n",
    "score_all = np.concatenate(score_all)\n",
    "\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.plot(score_all.T, '-o')\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "for j, xx in enumerate(M.testdata[idx_all]):\n",
    "    plt.subplot(1,len(idx_all),j+1)\n",
    "    plt.title('orig%s' % j)\n",
    "    plt.imshow(xx.numpy().reshape(28, 28), vmin=0, vmax=1)   \n",
    "    \n",
    "plt.figure(figsize=(20,5))\n",
    "for j, xx in enumerate(x_decod):\n",
    "    plt.subplot(1,len(idx_all),j+1)\n",
    "    plt.title('decod%s' % j)\n",
    "    plt.imshow(xx.detach().numpy().reshape(28, 28), vmin=0, vmax=1)   \n",
    "    \n",
    "\n",
    "# -underlay with the images of the characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## HERE: using the same i, but conditioning on each model differnetly\n",
    "## doesn't work - given some i, will give the same output each time... [for the decoder]. \n",
    "\n",
    "noiseval = 0\n",
    "sample_probs=True\n",
    "NiterNoise = 5\n",
    "score_all = []\n",
    "for n in range(NiterNoise):\n",
    "    ctmp, x_decod = model.sample(i=[idx_all[0] for _ in range(len(idx_all))], x=M.testdata[idx_all],\n",
    "                                sample_probs=sample_probs)\n",
    "    \n",
    "    _, score = model.decoder(i=np.arange(len(idx_all)), c=c_all, x=M.testdata[idx_all], \n",
    "                                   noise=noiseval, sample_probs=sample_probs)\n",
    "    score_all.append(score.detach().numpy())\n",
    "\n",
    "score_all = [ss.reshape(1,-1) for ss in score_all]\n",
    "score_all = np.concatenate(score_all)\n",
    "\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.plot(score_all.T, '-o')\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "for j, xx in enumerate(M.testdata[idx_all]):\n",
    "    plt.subplot(1,len(idx_all),j+1)\n",
    "    plt.title('orig%s' % j)\n",
    "    plt.imshow(xx.numpy().reshape(28, 28), vmin=0, vmax=1)   \n",
    "    \n",
    "plt.figure(figsize=(20,5))\n",
    "for j, xx in enumerate(x_decod):\n",
    "    plt.subplot(1,len(idx_all),j+1)\n",
    "    plt.title('decod%s' % j)\n",
    "    plt.imshow(xx.detach().numpy().reshape(28, 28), vmin=0, vmax=1)   \n",
    "    \n",
    "\n",
    "# -underlay with the images of the characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary so far:\n",
    "Am not sure how to \"refit\" a given model to different characters. The first method I tried (i.e. fix c, then decode conditioned on other characters) doesn't work, since c entirely determines the outpt. I think I need use the posterior for a given character (e.g. infer 10 of those guys to estimate the posterior, then sample from that). The plan of doing inference to get c, and then refitting that to all images does not work. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== clear all model mixtures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(i=np.arange(len(idx_all)), x=M.testdata[idx_all[0]].repeat(len(idx_all),1,1,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_probs = model(i=np.arange(len(idx_all)), x=M.testdata[idx_all])\n",
    "model(i=np.arange(len(idx_all))+3, x=M.testdata[idx_all[0]].repeat(len(idx_all),1,1,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_all\n",
    "x=M.testdata[j:j+1].repeat(100, 1, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.conditional(i=np.array(idx_all[0:1]), D=x_all[0:1], x=x_all[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_all = np.concatenate((idx_sample.reshape(1), idx_testmatch.reshape(1), idx_testnonmatch))\n",
    "\n",
    "a = [model.conditional(i=np.array(idx_all[0:1]), D=x_all[0:1], x=x_all[kk:kk+1]) for kk in range(1, 3)]\n",
    "print(a)\n",
    "\n",
    "a = np.concatenate([aa.detach().numpy() for aa in a])\n",
    "a = torch.tensor(a)\n",
    "print(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [model.conditional(i=np.array(idx_all[0:1]), D=x_all[0:1], x=x_all[kk:kk+1]) for kk in range(1, 3)]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ===== ZEROTH, extract indices in test set that you want to work with\n",
    "def getClassScore(charSamp, Nway=10, frontierSize=5, nUpdates=10, plotON=False):\n",
    "# charSamp = 3 # which digit to use for the sample?\n",
    "# Nway=15\n",
    "# frontierSize = 5; how many components to remember\n",
    "\n",
    "    # ========== GET RANDOM SAMPLES\n",
    "    # IN ORDER: (sample, same_as_sample, diff_from_sample)\n",
    "    idx_sample, idx_testmatch, idx_testnonmatch = getIdx(charSamp = charSamp, M=M, Nway=Nway)\n",
    "    idx_all = np.concatenate((idx_sample.reshape(1), idx_testmatch.reshape(1), idx_testnonmatch))\n",
    "\n",
    "\n",
    "    # EXTRACT DATA\n",
    "    x_samp = M.testdata[idx_all[0]] # sample\n",
    "#     x_test = [M.testdata[ii] for ii in idx_all[1:]] # need to already be updated in the model.\n",
    "    x_all = [M.testdata[ii] for ii in idx_all] # need to already be updated in the model.\n",
    "    # idx_all = range(len(idx_all))\n",
    "\n",
    "\n",
    "    # ====== FIRST, EMPTY MIXTURE COMPONENTS\n",
    "    model.frontierSize = frontierSize\n",
    "    from torch.nn import Parameter\n",
    "    model.mixtureComponents = [[] for _ in range(len(M.testdata))]\n",
    "    model.mixtureWeights = Parameter(torch.zeros(len(M.testdata), model.frontierSize)) #Unnormalised log-q\n",
    "    model.mixtureScores = [[] for _ in range(len(M.testdata))] #most recent log joint\n",
    "    model.nMixtureComponents = Parameter(torch.zeros(len(M.testdata)))\n",
    "\n",
    "\n",
    "    ## ===== FIRST, need to update model with posteriors for the novel stimuli    \n",
    "    if plotON is True:\n",
    "        print(model.mixtureWeights[idx_all[0]])\n",
    "    model.makeUpdates(i=idx_all, x=x_all, nUpdates=nUpdates)\n",
    "    if plotON is True:\n",
    "        print(model.mixtureWeights[idx_all[0]])\n",
    "\n",
    "    ## TESTING CLASSIFICATION ACCURACY\n",
    "    \n",
    "    # x_samp = M.testdata[idx_all[0]] # sample\n",
    "    # x_test = [M.testdata[ii] for ii in idx_all] # need to already be updated in the model.\n",
    "    # scores = model.conditional(idx_all, x_test, [x_samp]*(len(idx_all)-1))\n",
    "    scores = model.conditional(i=idx_all, D=x_all, x=[x_samp]*(len(idx_all))) # use model for D to predict x_samp\n",
    "    \n",
    "    scores_sampmodel = [model.conditional(i=np.array(idx_all[0:1]), D=x_all[0:1], x=x_all[kk:kk+1]) for kk in range(0, len(x_all))]\n",
    "    scores_sampmodel = torch.tensor(np.concatenate([aa.detach().numpy() for aa in scores_sampmodel]))\n",
    "\n",
    "    scores_sum = scores_sampmodel + scores\n",
    "    # ======== FOR EACH TEST CHARACTER, GET ITS SCORE \n",
    "    if plotON is True:\n",
    "        plt.figure()\n",
    "        plt.plot(scores.detach().numpy(), '-ok')\n",
    "        # predictive += scores[0].mean().item() #.item() so we don't hold onto tensor for gradient information\n",
    "        # if not (scores>scores[0]).any():\n",
    "        #     num_tied = (scores==scores[0]).sum().item()\n",
    "        #     hits += 1/num_tied \n",
    "        # total += 1\n",
    "\n",
    "        # print(\"Took\", int(time.time()-starttime), \"seconds\")\n",
    "        # print(hits / total) # hit rate\n",
    "        # print(predictive/total) # mean score\n",
    "    print(scores)\n",
    "    print(idx_all)\n",
    "    return scores, scores_sampmodel, idx_all, scores_sum\n",
    "\n",
    "hits = 0\n",
    "total = 0\n",
    "predictive = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ntrials = 1000\n",
    "import pickle\n",
    "charall = np.random.randint(0, 10, Ntrials)\n",
    "\n",
    "saveInterval = 20 # save mod this number trials\n",
    "\n",
    "# for w, f, u in zip([5, 5, 10, 10, 20, 20],[5, 10, 5, 10, 5, 10],[20, 20, 20, 20, 20, 20]):\n",
    "for w, f, u in zip([20, 20],[5, 10],[20, 20]):\n",
    "    scores_all = []\n",
    "    scores_sampmodel_all = []\n",
    "    scores_sum_all = []\n",
    "    idx_all = []\n",
    "    for i, cc in enumerate(charall):\n",
    "        print(i)\n",
    "        cc = np.array(cc)\n",
    "        scores, scores_sampmodel, idx, scores_sum = \\\n",
    "        getClassScore(charSamp=cc, Nway=w, frontierSize=f, nUpdates=u, plotON=False)\n",
    "\n",
    "        scores_all.append(scores.detach())\n",
    "        scores_sampmodel_all.append(scores_sampmodel.detach())\n",
    "        scores_sum_all.append(scores_sum.detach())\n",
    "        idx_all.append(idx)\n",
    "\n",
    "        if i%saveInterval==0 and i>0:\n",
    "            ## ========= SAVE OUTPUT\n",
    "            datall = [scores_all, scores_sampmodel, scores_sampmodel_all, idx_all]\n",
    "\n",
    "            # ======== SAVE\n",
    "            with open('./datsave/datall_w%sf%su%s' %(w, f, u), 'wb') as fl:\n",
    "                pickle.dump(datall, fl)\n",
    "\n",
    "            print('==== SAVED!')\n",
    "                \n",
    "                \n",
    "        ## ========= SAVE FINAL\n",
    "        datall = [scores_all, scores_sampmodel, scores_sampmodel_all, idx_all]\n",
    "        with open('./datsave/datall_w%sf%su%s' %(w, f, u), 'wb') as fl:\n",
    "            pickle.dump(datall, fl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w=5\n",
    "f=5\n",
    "u=20\n",
    "with open('./datsave/datall_w%sf%su%s' %(w, f, u), 'rb') as f:\n",
    "    scores_all, scores_sampmodel, scores_sampmodel_all, idx_all \\\n",
    "    = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w=5\n",
    "f=5\n",
    "u=20\n",
    "with open('./datsave/datall', 'rb') as f:\n",
    "    dat = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ============== OPTIONAL - LOAD PREVIOUSLY SAVED STUFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## SUMMARIZE CLASSIFICATION ERROR\n",
    "# ===== 1) PLOT likelihood probabilites\n",
    "plt.figure(figsize=(18, 5))\n",
    "\n",
    "for nn in range(10):\n",
    "    \n",
    "    plt.subplot(2,5,nn+1)\n",
    "    plt.title('int=%d' % nn)\n",
    "    \n",
    "#     scorestoplot = [scores_all[ii] for ii in range(len(idx_all)) if M.testlabels[idx_all[ii][0]]==nn] # trials with nn as trainsamp\n",
    "#     plt.plot(scorestoplot, '-ok')\n",
    "    \n",
    "    trialsthis = [ii for ii in range(len(idx_all)) if M.testlabels[idx_all[ii][0]]==nn] # trials with nn as trainsamp\n",
    "    \n",
    "    for tt in trialsthis:\n",
    "        plt.plot(scores_all[tt].detach().numpy(), '-')\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ===== PLOT - combine across integers\n",
    "plt.figure(figsize=(10, 5))\n",
    "for ss in scores_all:\n",
    "    plt.plot(ss.detach().numpy(), '-')\n",
    "plt.xlabel('train-test-rest')\n",
    "plt.ylabel('log(posterior predictive prob)')\n",
    "# ==== Histogram of \"rank\" for the correct test\n",
    "tmp = [sum(scores_all[i][1]>scores_all[i][2:]) for i in range(len(scores_all))]\n",
    "rank = np.array(len(scores_all[0])-2)-tmp # ranges from 0 to NumOthers\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.hist(rank, density=True)\n",
    "plt.title('rank of actual match (using sum of bidir probabilites)')\n",
    "plt.xlabel('rank(0==>best match)')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.hist(rank, density=False)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BElow - scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== GET RANDOM SAMPLES\n",
    "# IN ORDER: (sample, same_as_sample, diff_from_sample)\n",
    "idx_sample, idx_testmatch, idx_testnonmatch = getIdx(charSamp = charSamp, M=M, Nway=Nway)\n",
    "idx_all = np.concatenate((idx_sample.reshape(1), idx_testmatch.reshape(1), idx_testnonmatch))\n",
    "\n",
    "\n",
    "# EXTRACT DATA\n",
    "x_samp = M.testdata[idx_all[0]] # sample\n",
    "#     x_test = [M.testdata[ii] for ii in idx_all[1:]] # need to already be updated in the model.\n",
    "x_all = [M.testdata[ii] for ii in idx_all] # need to already be updated in the model.\n",
    "# idx_all = range(len(idx_all))\n",
    "\n",
    "\n",
    "# ====== FIRST, EMPTY MIXTURE COMPONENTS\n",
    "model.frontierSize = 5\n",
    "from torch.nn import Parameter\n",
    "model.mixtureComponents = [[] for _ in range(len(M.testdata))]\n",
    "model.mixtureWeights = Parameter(torch.zeros(len(M.testdata), model.frontierSize)) #Unnormalised log-q\n",
    "model.mixtureScores = [[] for _ in range(len(M.testdata))] #most recent log joint\n",
    "model.nMixtureComponents = Parameter(torch.zeros(len(M.testdata)))\n",
    "\n",
    "\n",
    "## ===== FIRST, need to update model with posteriors for the novel stimuli\n",
    "nUpdates=1\n",
    "print(model.mixtureWeights[idx_all[0]])\n",
    "model.makeUpdates(i=idx_all, x=x_all, nUpdates=nUpdates)\n",
    "print(model.mixtureWeights[idx_all[0]])\n",
    "\n",
    "## TESTING CLASSIFICATION ACCURACY\n",
    "\n",
    "hits = 0\n",
    "total = 0\n",
    "predictive = 0\n",
    "\n",
    "# x_samp = M.testdata[idx_all[0]] # sample\n",
    "# x_test = [M.testdata[ii] for ii in idx_all] # need to already be updated in the model.\n",
    "# scores = model.conditional(idx_all, x_test, [x_samp]*(len(idx_all)-1))\n",
    "scores = model.conditional(i=idx_all, D=x_all, x=[x_samp]*(len(idx_all)))\n",
    "# scores = model.conditional(range(len(x_all)), x_all, x_all)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(scores.detach().numpy(), '-ok')\n",
    "# predictive += scores[0].mean().item() #.item() so we don't hold onto tensor for gradient information\n",
    "# if not (scores>scores[0]).any():\n",
    "#     num_tied = (scores==scores[0]).sum().item()\n",
    "#     hits += 1/num_tied \n",
    "# total += 1\n",
    "\n",
    "# print(\"Took\", int(time.time()-starttime), \"seconds\")\n",
    "# print(hits / total) # hit rate\n",
    "# print(predictive/total) # mean score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ===== SECOND, get predictive probabilities\n",
    "# THIS IS DOING FOR TRAINING DATA...\n",
    "\n",
    "import time\n",
    "## HEWITT FUNCTION\n",
    "n_samples = 100\n",
    "n_way = 50\n",
    "\n",
    "n_samples = min(n_samples, len(M.data))\n",
    "n_way = min(n_way, len(M.data))\n",
    "print(\"Evaluating %d-way classification accuracy\"%n_way, flush=True)\n",
    "starttime=time.time()\n",
    "#i = list(range(len(testM.data)))\n",
    "\n",
    "hits = 0\n",
    "total = 0\n",
    "predictive = 0\n",
    "for trueclass in range(n_samples): #100 classes np.random.choice(range(len(testM.data)), size=500, replace=False):\n",
    "    x = M.data[trueclass]\n",
    "    others = list(range(trueclass)) + list(range(trueclass+1, len(M.data)))\n",
    "    i_rearranged = [trueclass] + list(np.random.choice(others, size=n_way-1, replace=False))\n",
    "    x_inp = [M.data[ii] for ii in i_rearranged] # need to already be updated in the model.\n",
    "    scores = model.conditional(i_rearranged, x_inp, [x]*n_way)\n",
    "    predictive += scores[0].mean().item() #.item() so we don't hold onto tensor for gradient information\n",
    "    #print(\"scores:\", scores)\n",
    "    if not (scores>scores[0]).any():\n",
    "        num_tied = (scores==scores[0]).sum().item()\n",
    "        hits += 1/num_tied \n",
    "    total += 1\n",
    "print(\"Took\", int(time.time()-starttime), \"seconds\")\n",
    "print(hits / total) # hit rate\n",
    "print(predictive/total) # mean score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
